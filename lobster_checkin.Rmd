---
title: "Trying to Reproduce Benestan et al. Assignment Results"
author: "Eric C. Anderson and Kelly Barr"
date: "Started: October 8, 2015"
output: 
  html_document:
    toc: true
---


Here we check some results that Louis Bernatchez's group had
on Lobster population genetics in this paper: 
[RAD genotyping reveals fine-scale genetic structuring and provides powerful population assignment in a widely distributed marine species, the American lobster (Homarus americanus)](http://onlinelibrary.wiley.com/doi/10.1111/mec.13245/abstract).

We found it strange that Benestan et al. found that their power for assignment decreased _dramatically_ as they added
more markers past 3000.  This seems worrisome to us because it is a pattern characteristic of
high grading bias.  Here we use Eric's program `gsi_sim` to implement the Training-Holdout-Leave-one-out
(THL) procedure from 
[Anderson 2010](http://onlinelibrary.wiley.com/doi/10.1111/j.1755-0998.2010.02846.x/full) 
to assess power for assignment of lobsters to their sampling
locations of origin.  We do not find as much power for assignment to sampling locations as Benestan and
colleagues report.  We conclude they suffered some high-grading bias in the procedure they used, even
though they intended to eliminate any such bias.

Following correspondence with Benestan, in which she confirmed that they had inadvertently included the training
individuals in their test set for assignment to sampling locations, we conducted analyses to assess how well
individuals could be assigned between North and South regions, effectively confirming that it is possible to 
assign individuals between North and South with accuracy in the low 90s of percent.  Interestingly, the power
of assignment continued to increase as markers were added up to the total number of markers, suggesting that
assigning individuals relies on small allele frequency differnces at many markers.


Note that if you want to reproduce these results you will want to be on a Mac or a Unix system.
You will need to have `vcftools` installed and in your `$PATH` variable which is given in a
`~/.bashrc` file.  Likewise you will need `gsi_sim`
([https://github.com/eriqande/gsi_sim](https://github.com/eriqande/gsi_sim) commit 280734b4 or later)
compiled and on your `$PATH`.  

To reproduce this work, clone this repo, install `gsi_sim` and `vcftools` and put them on your 
`$PATH` as listed in your `~/.bashrc`, then you have to download the file `10156-586.recode.vcf` from the
[Benestan et al. paper's Dryad site](http://datadryad.org/resource/doi:10.5061/dryad.q771r) and put it into
`./data/10156-586.recode.vcf` in the repository, and then open the [Rstudio](https://www.rstudio.com/)
project and knit the file `lobster_checkin.Rmd` to HTML within Rstudio.


## Setup
You gotta have the following libraries:
```{r, message=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
```

### Dealing with the individual IDs

First thing we need to do is correct some errors in the data set.  There are sampling locations codes in the
data set on Dryad that do not appear in the paper, and there are location codes in the paper that do not
appear in the data set.  Thierry, a coauthor of the paper, indicated that this could all be fixed by:

- Sampling sites EDN and CAP get renamed to MAG
- Sampling site SID gets renamed to DIN.

So, we are going to just do that and write out a corrected VCF file.  We put all the vcf output into a
directory called `intermediates`.
```{r}
dir.create("intermediates", showWarnings = FALSE)

# get the VCF file headers (indiv IDs, etc)
vcf_lines <- readLines("data/10156-586.recode.vcf")
header_idx <- which(str_detect(vcf_lines, "^#CHROM"))  # line has all the names
header_elements <- vcf_lines[header_idx] %>%
  str_split(., "\t") %>%
  "[["(.,1)

# now, rename the individuals as appropriate
ids <- header_elements[-(1:9)] %>%
  str_replace_all("EDN_", "MAG_a") %>%  # Note we append the a and b on the ID to keep them unique
  str_replace_all("CAP_", "MAG_b") %>%  # for individuals, but the same 3-letter location code
  str_replace_all("SID_", "DIN_")

# and put that new header back into there and print it out
vcf_lines[header_idx] <- paste(c(header_elements[1:9], ids), collapse = "\t")

cat(vcf_lines, file = "intermediates/10156-586-IDs-corrected.vcf", sep = "\n")

# then get a list of the individuals that are in there:
system("source ~/.bashrc; vcftools --vcf intermediates/10156-586-IDs-corrected.vcf --out intermediates/get-indivs --depth")

Inds <- read.table("intermediates/get-indivs.idepth", stringsAsFactors = FALSE, header = TRUE) %>%
  tbl_df %>%
  mutate(SampSite = str_sub(INDV, 1, 3))
```
Here is what `Inds` looks like:
```{r}
Inds
```
We don't need the mean read depths.  That was just a convenient way to list all the individuals in
the file.

Now, we are actually going to need some text files that list which individuals are from different
sampling locations.  We can do that like so:
```{r}
for(SS in unique(Inds$SampSite)) {
  Inds %>% 
    filter(SampSite == SS) %>%
    select(INDV) %>% 
    write.table(., file = file.path("intermediates", SS), row.names = FALSE, col.names = FALSE, quote = FALSE)
}
```


## Computing some Fst's
We are just going to compute Fst using VCF tools.  With multiple populations it computes
the "global" (not the
pairwise) Fst, which might not be what Benestan et al. did. 
It isn't entirely clear what Benestan et al. did, as they have only submitted data to 
Dryad and have not included any computer code documenting the analysis.  They said they
computed pairwise Fst using `hierfstat` but then you have $\frac{N(N-1)}{2}$ multiple values for each
locus (where N is the number of sampling location) so the
ranking method they would have used isn't immediately clear.  At any rate, using global
Fst should reasonably let you rank markers by how informative they are for some population
comparisons, since it will be correlated with pairwise Fst.  
```{r}
# first  make all the --weir-fst-pop commands
fst_comms <- paste("--weir-fst-pop", file.path("intermediates", unique(Inds$SampSite)), collapse = " ")

# then compute the Fst for all the markers
CALL <- paste("source ~/.bashrc; vcftools --vcf intermediates/10156-586-IDs-corrected.vcf --out intermediates/fst-all", fst_comms)
system(CALL)


# Now, read those in
FST <- read.table("intermediates/fst-all.weir.fst", header = TRUE, stringsAsFactors = FALSE) %>%
  tbl_df
```

Here is what they look like:
```{r}
FST
```

So, let's plot that and note that there are some loci that appear to be outliers (but that
could be the consequence of sampling variation, as well...)
```{r fstplot, fig.width=10, fig.height=7}
ggplot(FST, aes(x = WEIR_AND_COCKERHAM_FST)) + geom_histogram(binwidth = 0.001)
```


## Some functions to do THL (Training-Holdout-Leave-one-out)
We will write a few functions to do some things we need:
```{r}
# a function to turn a VCF file into a gsi_sim file.
# it will just take the name of the VCF input file and the output file
vcf2gsi_sim <- function(VCF, outf = "gs_baseline.txt") {
  # first output to a matrix of -1, 0, 1, and 2
  CALL <- paste("source ~/.bashrc; vcftools --vcf", VCF, "--out intermediates/tmp-one-two --012")
  system(CALL)
  
  # get the indivs
  gsIndivs <- scan("intermediates/tmp-one-two.012.indv", what = "character")
  N <- length(gsIndivs)
  
  # get the markers
  tmp <- scan("intermediates/tmp-one-two.012.pos", what = "character")
  # get their markers names by putting the Chromosome (always "un") and the
  # marker numbers together prepended with an X, so each will look like "X_un_3146850"
  gsMarkers <- paste("X", tmp[c(T,F)], tmp[c(F,T)], sep = "_")
  M <- length(gsMarkers)
  
  # now make a hashie thing to change some allele names and make it all two-columny
  alles <- c("1 1", "1 2", "2 2", "-1 -1")
  names(alles) <- c("0", "1", "2", "-1")
  
  # then read them in and make a matrix of "two-column genotypes
  genos <- alles[scan("intermediates/tmp-one-two.012", what = "character")] %>% 
    matrix(., nrow = N, byrow = TRUE)  
  
  rownames(genos) <- gsIndivs
  genos <- genos[,-1]  # note that we tear off the column that gives the index of each individual
  
  # now we need to sort the rows so that populations are together in case they arent already grouped
  # together.  Just a simple order on the rownames.
  genos <- genos[order(rownames(genos)),]
  
  
  # once we have done that we should modify the row names so that the first individual of each 
  # population has "POP NameOfPop\n" in its name, so we can print those out as they need to be
  # for gsi_sim.  Note that this is specific to the naming format on the lobster data
  pops <- str_sub(rownames(genos), 1, 3)
  firsties <- c(1, 1 + which(diff(as.numeric(factor(pops))) != 0))
  rownames(genos)[firsties] <- paste("POP ", pops[firsties], "\n", rownames(genos)[firsties], sep = "")
  
  
  # then we just spooge out the preamble and the genotypes.
  cat(nrow(genos), " ", ncol(genos), "\n", sep = "", file = outf)
  cat(gsMarkers, sep = "\n", file = outf, append = TRUE)
  write.table(genos, col.names = FALSE, quote = FALSE, sep = "    ", file = outf, append = TRUE)
  
  # return the name of the file written to:
  return(outf)
}

# from gsi_sim --self-assign output, slurp out the population each individual
# got assigned to, and note whether it is correct or not.
# this relies on having pop names that are exactly three letters long.
slurp_ass <- function(infile = "gs_out.txt") {
  tmp <- readLines(infile)
  tmp2 <- tmp[str_detect(tmp, "^SELF_ASSIGN_A_LA_GC_CSV")] %>%
    str_replace_all(., "^SELF_ASSIGN_A_LA_GC_CSV:/", "")
  str_split(tmp2, ";") %>% 
    sapply(., "[", 1:3) %>% 
    t %>%
    as.data.frame(stringsAsFactors = FALSE) %>%
    tbl_df %>%
    setNames(c("ID", "To", "Score")) %>%
    mutate(From = str_sub(ID, 1, 3))
}
```

## Split the data into Training and Test sets
We will just take (as close as possible) half of the individuals, randomly selected, to be
Training data, and the other half to be our Test (holdout) individuals.  We will identify who
they are and store that info in the variable `Split`
```{r}
set.seed(345) # for reproducibility
# permute them within sample location and then let every other one be a Training individual 
Split <- Inds %>%
  group_by(SampSite) %>%
  mutate(Pool = ifelse(sample(1:length(INDV)) %% 2 == 0, "Test", "Train")) %>%
  ungroup
```
Here is what `Split` looks like:
```{r}
Split
```

Then we make two new VCF files:
one of the Training individuals and one of the Test individuals.
```{r}
# first we make files with the names of the individuals
Split %>% 
  filter(Pool == "Test") %>% 
  select(INDV) %>% 
  unlist %>% 
  unname  %>%
  cat(., file = "intermediates/test-guys.txt", sep = "\n")

Split %>% 
  filter(Pool == "Train") %>% 
  select(INDV) %>% 
  unlist %>% 
  unname  %>%
  cat(., file = "intermediates/train-guys.txt", sep = "\n")

# then we run vcftools to grab those out:
system("source ~/.bashrc; vcftools --vcf intermediates/10156-586-IDs-corrected.vcf --keep intermediates/train-guys.txt --out intermediates/train --recode")
system("source ~/.bashrc; vcftools --vcf intermediates/10156-586-IDs-corrected.vcf --keep intermediates/test-guys.txt --out intermediates/test --recode")
```


## Compute FST via vcftools using just the Training data


Compute Fst's from the training set with vcftools.  The results are written to a file which
we then read into `train_FST` which has things sorted in descending order of FST:
```{r}
fst_comms <- paste("--weir-fst-pop", file.path("intermediates", unique(Inds$SampSite)), collapse = " ")
CALL <- paste("source ~/.bashrc; vcftools --vcf intermediates/train.recode.vcf --out intermediates/train-recode", fst_comms)
system(CALL)


# get a table of marker positions sorted by Fst high to low
train_FST <- read.table("intermediates/train-recode.weir.fst", header = TRUE, stringsAsFactors = FALSE) %>%
  tbl_df %>%
  arrange(desc(WEIR_AND_COCKERHAM_FST))
```
Here is what `train_FST` looks like:
```{r}
train_FST
```

## A function to do the THL method on a given number, nL, of loci

We are now almost there, but it will be convenient to have a function to which you can pass:

1. The number of loci to select
1. A data frame `Split` that tells us who is Training and who is Test
1. A data frame like `train_FST`
1. A VCF file with all the individuals and all the SNPs

And it will return to you a data frame telling you whether each individual was correctly assigned to
its sampling location or not (and also whether or not it was a Training or a Test individual.
For this to work, the ID's of the individuals all have to start with
a three-letter location code.
```{r}
thl <- function(nL, Split, train_FST, vcffile = "intermediates/10156-586-IDs-corrected.vcf") {
  # now, choose the first nL loci, write them to a file, and get all the individuals'
  # genotypes at those loci
  fn <- file.path("intermediates", "tmp_top_loci")
  train_FST[1:nL, ] %>%
    select(CHROM, POS) %>%
    write.table(., row.names = FALSE, col.names = FALSE, sep = "\t",
                quote = FALSE, file = fn)
  
  
  CALL <- paste("source ~/.bashrc; vcftools --vcf",
                vcffile,
                "--positions",
                fn, 
                "--recode --out",
                fn
  )
  
  system(CALL)
  
  
  # make a gsi_sim file of that:
  vcf2gsi_sim(paste(fn, "recode.vcf", sep = "."), 
              outf = paste(fn, "gsi_sim_input", sep = "."))
  
  # then run gsi_sim on it
  system(paste("source ~/.bashrc; gsi_sim -b", paste(fn, "gsi_sim_input", sep = "."),
               "--self-assign > intermediates/bigdump"))
  
  
  # and slurp out the assignments
  result <- slurp_ass(file.path("intermediates", "bigdump"))
  
  # join back onto these whether or not they were Train or Test and add a "Correct" column
  # and also a number of loci column.  And return that data frame.
  result %>%
    left_join(Split, ., by = c("INDV" = "ID")) %>%
    mutate(Correct = (From == To),
           NumLoci = nL)
}
```

## Finally, let's do some experiments

### Selecting loci on the basis of vcftools-calculated global FST

We do the experiment with the same numbers of loci as done in the original paper.
```{r, cache=TRUE}
# number of SNPs
LocNums <- c(500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 10156)

# do them all...
ResList <- lapply(LocNums, function(x) thl(x, Split, train_FST))

# then make a big tidy data frame of it:
ResDF <- bind_rows(ResList)

# compute the proportion correct from each population under each scenario of
# NumLoci and Train/Test
SumDF <- ResDF %>%
  group_by(NumLoci, From, Pool) %>%
  summarize(NumCorrect = sum(Correct),
            Ppn_Correct = NumCorrect / n())
```
Here is what that result data set looks like:
```{r}
SumDF
```
Note that doing THL correctly requires that one focus only on the Test individuals to calculate the proportion of
correct assignment.  If you use only the training individuals or if you use both the training and the test individuals
then that will give you upwardly biased predictions of the power for assignment.  It will actually be worth seeing what the
result looks like if you use both the Training and the Test individuals to assess the proportion of correct assignments.
We can do that like this by labeling those individuals as "Both," summarizing, then appending to SumDF.
```{r}
tmpDF <- ResDF
tmpDF$Pool <- "Both"
NewSumDF <- tmpDF %>%
  group_by(NumLoci, From, Pool) %>%
  summarize(NumCorrect = sum(Correct),
            Ppn_Correct = NumCorrect / n()) %>%
  bind_rows(SumDF, .) %>%
  ungroup
```
It might be instructive to see what that data frame looks like:
```{r}
NewSumDF
```
And now we can can plot boxplots from those results. X-axis is number of loci 
(as factor), and Y-axis is proportion of correct
self-assignment in each sampling location.  In other words, each boxplot 
summarizes the results over all the
different sampling locations.

The different colors denote what happens when you use different sets of individuals as the
"Test" set, i.e. either the Holdout individuals (correct, labeled "Test" in the figure) or
the Training individuals
(incorrect and subject to high grading bias) or Both the Training and the Holdout individuals
(also incorrect and subject to high grading bias).


```{r boxplots, fig.width=10, fig.height=7}
ggplot(NewSumDF, aes(x = factor(NumLoci), y = Ppn_Correct, fill = Pool)) +
  geom_boxplot()
```

So, when we assess these markers using THL with selection based on global Fst as computed by
`vcftools` we find that there isn't that much power for identifying different sampling locations, and
the pattern observed by Benestan et al. looks a lot like what we see with high-grading bias.
We suspect there was an error in Benestan et al.'s analysis.

We sent them this analysis and Laura followed up, confirming that there was a problem with her analysis.

## Following up on Benestan's proposed erratum

Laura got back to us with a proposed erratum.  We now check up on some of her numbers here.  

### Population assignment the way they did it

Apparently, Laura accidentally used all the individuals as a training set and then used a random half of
those individuals as the holdout/test set.  So, let's see if we can reproduce her results by doing that.

Our variable `FST` holds the ranking of the loci by FST when using everyone as the training data.  So, we 
can just substitute that into our function, using the same individuals as before as the test data (which
were not properly held out!).  Note that we have to sort the FST results to use them
```{r bogusTrain}
# arrange FST from all individuals
bogus_trainFST <- FST %>%
  arrange(desc(WEIR_AND_COCKERHAM_FST))
```
Check out the top loci in this...Interestingly the loci with the highest Fst's here don't seem to intersect
with those from `trainFST`. I suppose that this suggests that the much of the variation across loci 
(at least in the upper tail of the distribution) in FST
is driven by sampling variation, rather than location-specific differences.
```{r}
bogus_trainFST
```

Now, go ahead and do the runs
```{r like_Bene, cache=TRUE}
# do them all...
ResList_bene <- lapply(LocNums, function(x) thl(x, Split, bogus_trainFST))

# then make a big tidy data frame of it:
ResDF_bene <- bind_rows(ResList_bene)

# compute the proportion correct from each population under each scenario of
# NumLoci and Train/Test
SumDF_bene <- ResDF_bene %>%
  group_by(NumLoci, From, Pool) %>%
  summarize(NumCorrect = sum(Correct),
            Ppn_Correct = NumCorrect / n())
```
And after that we plot it:
```{r, fig.width=10, fig.height=7}
ggplot(SumDF_bene, aes(x = factor(NumLoci), y = Ppn_Correct, fill = Pool)) +
  geom_boxplot()
```

OK, that is what we might expect it to look like, and it looks a lot like the result that
Benestan et al. got. 

However, when Laura did it over again recently (in a more correct fashion),
she reported correct assignment to population of 
39.4%, which is almost twice what we see here.  So, I suspect that she still has an error in what she
did.

### Assignment to regional groups using the population assignments
The erratum that Laura sent me claims that the assignment to regional groups does not suffer from
a very noticeable high grading bias and the true assignment success to north and south should be 
correct, about 94.5\% of the time.  

I want to do a quick check on that just using the sampling-location-level assignments, from which
we will toss the individuals into separate "North" and "South" populations.  I don't think that 
this is how Laura did it, but it is a common approach to dealing with "Reporting Units" in the salmon
world, and we can do it using the results we have already computed.

So, let's get a data frame with the regional groups and then define the regions in ResDF.  We made a data
frame of the North/South membership of different locations (from the paper) and saved it into
`data/RegionsAndPops.rds`.

```{r}
Pop_n_reg <- readRDS("data/RegionsAndPops.rds")
RegionalDF <- left_join(ResDF, Pop_n_reg, by = c("From" = "Pop")) %>%
  rename(FromRegion = Region) %>%
  left_join(., Pop_n_reg, by = c("To" = "Pop")) %>%
  rename(ToRegion = Region) %>%
  mutate(RegionCorrect = FromRegion == ToRegion)

# and summarize too
RegionalSummary <- RegionalDF %>%
  group_by(NumLoci, From, Pool) %>%
  summarize(NumCorrect = sum(RegionCorrect),
            Ppn_Correct = NumCorrect / n())
```
Then we can plot that:
```{r, fig.width=10, fig.height=7}
ggplot(RegionalSummary, aes(x = factor(NumLoci), y = Ppn_Correct, fill = Pool)) +
  geom_boxplot()
```
So, if you break everything into separate populations and then group them into
"lobster regions", you are doing pretty well with all the markers, but not as well with
3000 markers.  Pretty much just what you would expect.

Interestingly, this shows that even with the fairly small samples at each 
sampling location, there is enough information about allele frequencies to make that
information useful to assign individuals to the correct region at a rate of about 
90%, even if you can't assign them to the correct sampling location.  

However, one might suspect taht you could do better by pooling all the regional samples so that the sample
sizes are large for North and South. In effect considering that the lobsters come from just 
two populations: North or South.  We will explore this, as it appears to be what
Benestan et al. did.




### Assignment to regional groups using regional groups as populations
The way we have hacked this together, it is going to be easiest to change the original VCF file so that
we can still use all the same functions.  We will just prepend "South" or "North" to the ID of each
individuals, which will give us "Populations" of "Sou" and "Nor".
```{r}
vcf_lines <- readLines("intermediates/10156-586-IDs-corrected.vcf")
header_idx <- which(str_detect(vcf_lines, "^#CHROM"))  # line has all the names
header_elements <- vcf_lines[header_idx] %>%
  str_split(., "\t") %>%
  "[["(.,1)

# now we want to prepend a North or a South to each if appropriate:
ids_df <- data.frame(ID = header_elements[-(1:9)], stringsAsFactors = FALSE) %>%
  tbl_df %>%
  mutate(Pop = str_sub(ID, 1, 3)) %>% 
  left_join(Pop_n_reg) %>%
  mutate(NewID = paste(Region, ID, sep = "_"))

# now we make our new header line and write that out into a new VCF file
new_header_line <- c(header_elements[1:9], ids_df$NewID) %>%
  paste(., collapse = "\t")

vcf_lines[header_idx] <- new_header_line

cat(vcf_lines, file = "intermediates/north_south_renamed.vcf", sep = "\n")
```

OK, now `intermediates/north_south_renamed.vcf` is a file that we can just pump through the steps we
took above for assessing high-grading bias. Let's make a variable like Inds:
```{r}
system("source ~/.bashrc; vcftools --vcf intermediates/north_south_renamed.vcf --out intermediates/get-indivs-reg --depth")

IndsReg <- read.table("intermediates/get-indivs-reg.idepth", stringsAsFactors = FALSE, header = TRUE) %>%
  tbl_df %>%
  mutate(SampSite = str_sub(INDV, 1, 3))
```
Now `SampSite` takes values of `Nor` or `Sou`.

### Split these into regional training and test sets
Same drill as before:
```{r}
set.seed(555) # for reproducibility
# permute them within sample location and then let every other one be a Training individual 
SplitReg <- IndsReg %>%
  group_by(SampSite) %>%
  mutate(Pool = ifelse(sample(1:length(INDV)) %% 2 == 0, "Test", "Train")) %>%
  ungroup
```

Now, make two vcf files with those different pools in them:
```{r}
# first we make files with the names of the individuals
SplitReg %>% 
  filter(Pool == "Test") %>% 
  select(INDV) %>% 
  unlist %>% 
  unname  %>%
  cat(., file = "intermediates/test-guys-reg.txt", sep = "\n")

SplitReg %>% 
  filter(Pool == "Train") %>% 
  select(INDV) %>% 
  unlist %>% 
  unname  %>%
  cat(., file = "intermediates/train-guys-reg.txt", sep = "\n")

# then we run vcftools to grab those out:
system("source ~/.bashrc; vcftools --vcf intermediates/north_south_renamed.vcf --keep intermediates/train-guys-reg.txt --out intermediates/train-reg --recode")
system("source ~/.bashrc; vcftools --vcf intermediates/north_south_renamed.vcf --keep intermediates/test-guys-reg.txt --out intermediates/test-reg --recode")

```

Then compute Fst from the training data:
```{r}
# first make a file that names the north and south individuals 
IndsReg %>%
  filter(str_detect(INDV, "North")) %>% 
  select(INDV) %>%
  unlist %>%
  unname %>%
  cat(., file = "intermediates/North", sep = "\n")

IndsReg %>%
  filter(str_detect(INDV, "South")) %>% 
  select(INDV) %>%
  unlist %>%
  unname %>%
  cat(., file = "intermediates/South", sep = "\n")


# then put together the commands to compute the Fst's using the training individuals
fst_comms <- paste("--weir-fst-pop", file.path("intermediates", c("North", "South")), collapse = " ")
CALL <- paste("source ~/.bashrc; vcftools --vcf intermediates/train-reg.recode.vcf --out intermediates/train-recode-reg", fst_comms)
system(CALL)

```

Let's have a look at the Fst between regions computed from the training data:
```{r, fig.width=10, fig.height=7}
trainFST_reg <- read.table("intermediates/train-recode-reg.weir.fst", 
                           stringsAsFactors = FALSE, 
                           header = TRUE) %>%
  tbl_df

ggplot(trainFST_reg, aes(x = WEIR_AND_COCKERHAM_FST)) +
  geom_histogram(binwidth = 0.001)
```


At any rate, we are now ready to run this.  
```{r, cache=TRUE}
# do them all...
ResListReg <- lapply(LocNums, function(x) thl(nL = x, 
                                              Split = SplitReg, 
                                              train_FST = trainFST_reg, 
                                              vcffile = "intermediates/north_south_renamed.vcf"))

ResDF_reg <- bind_rows(ResListReg)

# compute the proportion correct from each population under each scenario of
# NumLoci and Train/Test
SumDF_reg <- ResDF_reg %>%
  group_by(NumLoci, From, Pool) %>%
  summarize(NumCorrect = sum(Correct),
            Ppn_Correct = NumCorrect / n())
```

So, now we can plot those as lines, faceted on North and South:
```{r, fig.width=10, fig.height=7}
ggplot(SumDF_reg, aes(x = NumLoci, y = Ppn_Correct, colour = Pool)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ From, nrow = 1)
```

The "Train" lines show the results when the training set is used as the test set (and thus would be subject to high grading bias).  This indicates that with the larger sample sizes there is not a strong high grading bias effect (as stated in Anderson 2010, the effect will be larger with small samples.)  And, it shows that using all the loci one can apparently allocate individuals to region with with accuracy > 90%.  

It is also important to note that the assignment probability increases with each marker.  There are two 
important conclusions we can draw from this:

- It is not the case that you can do better (or even just as well) with a subset of markers in this case---you
  can use every additional marker.
- The power for assignment here might be an overestimate because the VCF file provided on Dryad does not seem
  to indicate which markers occur within the same RAD locus.  Thus, many of the SNPs might be within 100 bp of
  one another, and hence in high LD.  `gsi_sim` assumes that markers are not in LD in the separate 
  populations, so, if they are, then it will
  overestimate the power for assignment.



## A quick sanity check

Here is a quick sanity check that could be done: permute the individuals in the data setand randomly allocate them
to two different groups: Fake1 and Fake2, say, of the same size as the North and South and then
see how well we can allocate them.  I expect, not at all, but it is worth making sure that we don't have
some weird artifactual thing going on.  I should really roll the whole process above into a function, but 
I don't want to hassle with that with all the text files that are getting written, etc.  I 
am just going to lazily copy and paste everything from above and tweak:

```{r}
vcf_lines <- readLines("intermediates/10156-586-IDs-corrected.vcf")
header_idx <- which(str_detect(vcf_lines, "^#CHROM"))  # line has all the names
header_elements <- vcf_lines[header_idx] %>%
  str_split(., "\t") %>%
  "[["(.,1)

# now we want to prepend a FK1 or a FK2 (Fake-1 or Fake-2) to each if appropriate.
ids_df <- data.frame(ID = header_elements[-(1:9)], stringsAsFactors = FALSE) %>%
  tbl_df %>%
  mutate(Pop = str_sub(ID, 1, 3)) %>% 
  left_join(Pop_n_reg) 
# First, figure out how many North and South individuals there are:
nums <- ids_df %>%
  group_by(Region) %>%
  tally() %>%
  select(n) %>%
  unlist
# then make a series of those prefixes
name_prepend <- sample(c(rep("FK1", nums[1]), rep("FK2", nums[2])))



# we get a file of individual names that we want and print it out
ids_df <- ids_df %>%
  mutate(NewID = paste(name_prepend, ID, sep = "_")) 

ids_df %>%
  filter(str_detect(NewID, "FK1") | str_detect(NewID, "FK2")) %>%
  arrange(NewID) %>%
  select(NewID) %>% 
  unlist %>%
  unname %>%
  cat(., sep = "\n", file = "intermediates/north_south_names.txt") 

# now we make our new header line and write that out into a new VCF file
new_header_line <- c(header_elements[1:9], ids_df$NewID) %>%
  paste(., collapse = "\t")

vcf_lines[header_idx] <- new_header_line

cat(vcf_lines, file = "intermediates/north_south_renamed.vcf", sep = "\n")
```

OK, now `intermediates/north_south_renamed.vcf` is a file that we can just pump through the steps we
took above for assessing high-grading bias. Let's make a variable like Inds:
```{r}
system("source ~/.bashrc; vcftools --vcf intermediates/north_south_renamed.vcf --out intermediates/get-indivs-reg --depth")

IndsReg <- read.table("intermediates/get-indivs-reg.idepth", stringsAsFactors = FALSE, header = TRUE) %>%
  tbl_df %>%
  mutate(SampSite = str_sub(INDV, 1, 3))
```
Now `SampSite` takes values of `Nor` or `Sou`.

### Split these fake groups into regional training and test sets
Same drill as before:
```{r}
set.seed(111) # for reproducibility
# permute them within sample location and then let every other one be a Training individual 
SplitReg <- IndsReg %>%
  group_by(SampSite) %>%
  mutate(Pool = ifelse(sample(1:length(INDV)) %% 2 == 0, "Test", "Train")) %>%
  ungroup
```

Now, make two vcf files with those different pools in them:
```{r}
# first we make files with the names of the individuals
SplitReg %>% 
  filter(Pool == "Test") %>% 
  select(INDV) %>% 
  unlist %>% 
  unname  %>%
  cat(., file = "intermediates/test-guys-reg.txt", sep = "\n")

SplitReg %>% 
  filter(Pool == "Train") %>% 
  select(INDV) %>% 
  unlist %>% 
  unname  %>%
  cat(., file = "intermediates/train-guys-reg.txt", sep = "\n")

# then we run vcftools to grab those out:
system("source ~/.bashrc; vcftools --vcf intermediates/north_south_renamed.vcf --keep intermediates/train-guys-reg.txt --out intermediates/train-reg --recode")
system("source ~/.bashrc; vcftools --vcf intermediates/north_south_renamed.vcf --keep intermediates/test-guys-reg.txt --out intermediates/test-reg --recode")

```

Then compute Fst from the training data:
```{r}
# first make a file that names the north and south individuals 
IndsReg %>%
  filter(str_detect(INDV, "FK1")) %>% 
  select(INDV) %>%
  unlist %>%
  unname %>%
  cat(., file = "intermediates/North", sep = "\n")

IndsReg %>%
  filter(str_detect(INDV, "FK2")) %>% 
  select(INDV) %>%
  unlist %>%
  unname %>%
  cat(., file = "intermediates/South", sep = "\n")


# then put together the commands to compute the Fst's using the training individuals
fst_comms <- paste("--weir-fst-pop", file.path("intermediates", c("North", "South")), collapse = " ")
CALL <- paste("source ~/.bashrc; vcftools --vcf intermediates/train-reg.recode.vcf --out intermediates/train-recode-reg", fst_comms)
system(CALL)

```

Let's have a look at the Fst between regions computed from the training data:
```{r}
trainFST_reg <- read.table("intermediates/train-recode-reg.weir.fst", 
                           stringsAsFactors = FALSE, 
                           header = TRUE) %>%
  tbl_df

ggplot(trainFST_reg, aes(x = WEIR_AND_COCKERHAM_FST)) +
  geom_histogram(binwidth = 0.001)
```

Hmm...the Fst between these two groups that got computed there seems lower than what is reported in the paper.

At any rate, we are now ready to run this.  
```{r, cache=TRUE}
# number of SNPs
LocNums <- c(25, 50, 100, 250, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 10156)

# do them all...
ResListReg <- lapply(LocNums, function(x) thl(nL = x, 
                                              Split = SplitReg, 
                                              train_FST = trainFST_reg, 
                                              vcffile = "intermediates/north_south_renamed.vcf"))

ResDF_reg <- bind_rows(ResListReg)

# compute the proportion correct from each population under each scenario of
# NumLoci and Train/Test
SumDF_reg <- ResDF_reg %>%
  group_by(NumLoci, From, Pool) %>%
  summarize(NumCorrect = sum(Correct),
            Ppn_Correct = NumCorrect / n())
```

So, now we can plot those as lines, faceted on FK1 and FK2:
```{r}
ggplot(SumDF_reg, aes(x = NumLoci, y = Ppn_Correct, colour = Pool)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ From, nrow = 1)
```

